{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10717700-3938-41f3-99e6-a989a064ccae",
   "metadata": {},
   "source": [
    "# Assignment 19 'Machine Learning'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43faf558-1e6c-4c96-953c-4124c9090986",
   "metadata": {},
   "source": [
    "### 1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2 and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
    "\n",
    "a) Using the k-means method, create two clusters for each set of centroid described above.\n",
    "\n",
    "b) For each set of centroid values, calculate the SSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58989443-fbe5-4708-b45b-0228f125739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of cluster {10, 20, 5, 15} : 12.5\n",
      "Mean of cluster {25, 35, 30} : 30.0\n",
      "SSE_1 of Cluster : 175.0\n",
      "********************************************************************************************************\n",
      "Mean of cluster {10, 5, 15} : 10.0\n",
      "Mean of cluster {25, 35, 20, 30} : 27.5\n",
      "SSE_2 of Cluster : 175.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "d={5,10,15,20,25,30,35}\n",
    "K=2\n",
    "#With K = 2, the initial clusters we arrived at are {5,10,15,20} and {25,30,35}.\n",
    "c1= {5,10,15,20}\n",
    "c2= {25,30,35}\n",
    "\n",
    "mean_c1= np.mean(list(c1))\n",
    "print(f\"Mean of cluster {c1} :\",mean_c1)\n",
    "\n",
    "mean_c2= np.mean(list(c2))\n",
    "print(f\"Mean of cluster {c2} :\",mean_c2)\n",
    "\n",
    "#So, the SSE within the clusters is\n",
    "SSE_1=(5-mean_c1)**2 + (10-mean_c1)**2 + (15-mean_c1)**2 + (20-mean_c1)**2 + (25-mean_c2)**2 + (30-mean_c2)**2 + (35-mean_c2)**2\n",
    "\n",
    "print(\"SSE_1 of Cluster :\",SSE_1)\n",
    "\n",
    "# If we compare this with the cluster {5,10,15} and {20,25,30,35\n",
    "print(\"********************************************************************************************************\")\n",
    "c1= {5,10,15}\n",
    "c2= {20,25,30,35}\n",
    "\n",
    "mean_c1= np.mean(list(c1))\n",
    "print(f\"Mean of cluster {c1} :\",mean_c1)\n",
    "\n",
    "mean_c2= np.mean(list(c2))\n",
    "print(f\"Mean of cluster {c2} :\",mean_c2)\n",
    "\n",
    "#So, the SSE within the clusters is\n",
    "SSE_2=(5-mean_c1)**2 + (10-mean_c1)**2 + (15-mean_c1)**2 + (20-mean_c2)**2 + (25-mean_c2)**2 + (30-mean_c2)**2 + (35-mean_c2)**2\n",
    "\n",
    "print(\"SSE_2 of Cluster :\",SSE_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded8aff6-1eaa-4a92-aaf2-097177196940",
   "metadata": {},
   "source": [
    "#### 2. Describe how the Market Basket Research makes use of association analysis concepts.\n",
    "\n",
    "Association rule presents a methodology that is useful for\n",
    "identifying interesting relationships hidden in large data\n",
    "sets. It is also known as `association analysis`, and the\n",
    "discovered relationships can be represented in the form\n",
    "of association rules comprising a set of frequent items. A\n",
    "common application of this analysis is the `Market\n",
    "Basket Analysis` that retailers use for cross-selling of\n",
    "their products. \n",
    "\n",
    "**For example,** every large grocery store\n",
    "accumulates a large volume of data about the buying\n",
    "pattern of the customers. On the basis of the items purchased together, the retailers can push some crossselling\n",
    "either by placing the items bought together in\n",
    "adjacent areas or creating some combo offer with those\n",
    "different product types. The below association rule\n",
    "signifies that people who have bought bread and milk\n",
    "have often bought egg also; so, for the retailer, it makes\n",
    "sense that these items are placed together for new\n",
    "opportunities for cross-selling.\n",
    "\n",
    "`{Bread, Milk} → {Egg}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e1ff3-e8dc-4bdb-a740-9e728a650802",
   "metadata": {},
   "source": [
    "### 3. Give an example of the Apriori algorithm for learning association rules.\n",
    "\n",
    "If a seller is dealing with 100 different\n",
    "items, then the learner need to evaluate 2 = 1 × e\n",
    "itemsets for arriving at the rule, which is computationally\n",
    "impossible. So, it is important to filter out the most\n",
    "important (and thus manageable in size) itemsets and\n",
    "use the resources on those to arrive at the reasonably\n",
    "efficient association rules.\n",
    "\n",
    "The first step for us is to decide the minimum support\n",
    "and minimum confidence of the association rules. From\n",
    "a set of transaction T, let us assume that we will find out\n",
    "all the rules that have support ≥ minS and confidence ≥\n",
    "minC, where minS and minC are the support and\n",
    "confidence thresholds, respectively, for the rules to be\n",
    "k\n",
    "\n",
    "100 30\n",
    "\n",
    "considered acceptable. Now, even if we put the minS =\n",
    "20% and minC = 50%, it is seen that more than 80% of\n",
    "the rules are discarded; this means that a large portion of\n",
    "the computational efforts could have been avoided if the\n",
    "itemsets for consideration were first pruned and the\n",
    "itemsets which cannot generate association rules with\n",
    "reasonable support and confidence were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5636c4cf-e228-458c-bd32-0f89923f6282",
   "metadata": {},
   "source": [
    "### 4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric is used to decide when to end the iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112cbafa-1961-4bfd-a1e2-8b593bd771ed",
   "metadata": {},
   "source": [
    "There are two main `hierarchical clustering` methods: \n",
    "* agglomerative clustering and \n",
    "* divisive clustering.\n",
    "\n",
    "`Agglomerative clustering `is a\n",
    "bottom-up technique which starts with individual objects as clusters\n",
    "and then iteratively merges them to form larger clusters. On the\n",
    "other hand, \n",
    "\n",
    "`the divisive method` starts with one cluster with all given\n",
    "objects and then splits it iteratively to form smaller clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2e2f3-9a46-49f7-b30c-536b69b9ccdc",
   "metadata": {},
   "source": [
    "### 5. In the k-means algorithm, how do you recompute the cluster centroids?\n",
    "\n",
    "The hierarchical clustering technique on sample points\n",
    "from the data set and then arrive at sample K clusters.\n",
    "The centroids of these initial K clusters are used as the\n",
    "initial centroids. This approach is practical when the data\n",
    "set has small number of points and K is relatively small\n",
    "compared to the data points. There are procedures such\n",
    "as bisecting k-means and use of post-processing to fix\n",
    "initial clustering issues; these procedures can produce\n",
    "better quality initial centroids and thus better SSE for\n",
    "the final clusters.\n",
    "\n",
    "Elbow point to determine the appropriate number of clusters\n",
    "Recomputing cluster centroids\n",
    "\n",
    "We discussed in the k-means algorithm that the iterative\n",
    "step is to recalculate the centroids of the data set after\n",
    "each iteration. The proximities of the data points from\n",
    "each other within a cluster is measured to minimize the\n",
    "distances. The distance of the data point from its nearest\n",
    "centroid can also be calculated to minimize the distances\n",
    "to arrive at the refined centroid. The Euclidean distance\n",
    "between two data points is measured as follows:\n",
    "Using this function, the distance between the example\n",
    "data and its nearest centroid and the objective is\n",
    "calculated to minimize this distance. The measure of\n",
    "quality of clustering uses the SSE technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf03c89-800d-4997-8640-27bc235e4a5a",
   "metadata": {},
   "source": [
    "### 6. At the start of the clustering exercise, discuss one method for determining the required number of clusters.\n",
    "\n",
    "k-means often produce local optimum and not global\n",
    "optimum. Also, the result depends on the initial selection\n",
    "of random cluster centroids. It is a common practice to\n",
    "run the k-means algorithm multiple times with different\n",
    "cluster centres to identify the optimal clusters. The\n",
    "necessity to set the initial ‘K’ values is also perceived as a\n",
    "disadvantage of the k-means algorithm. There are\n",
    "methods to overcome this problem, such as defining a\n",
    "range for ‘K’ and then comparing the results of clustering\n",
    "with those different ‘K’ values to arrive at the best\n",
    "possible cluster. The ways to improve cluster\n",
    "performance is an area of further study, and many\n",
    "different techniques are employed to achieve that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b52f55-a973-4e84-8c6e-92da39892a14",
   "metadata": {},
   "source": [
    "### 7. Discuss the k-means algorithm&#39;s advantages and disadvantages.\n",
    "\n",
    "k-Means: Advantages and Disadvantages \n",
    "\n",
    "Advantages\t\n",
    "\n",
    "• Easy\tto\timplement\t\n",
    "• With\ta\tlarge\tnumber\tof\tvariables,\tK-Means\tmay\tbe\tcomputaHonally\tfaster\tthan\t\n",
    "hierarchical\tclustering\t(if\tK\tis\tsmall).\t\n",
    "• k-Means\tmay\tproduce\tHghter\tclusters\tthan\thierarchical\tclustering\t\n",
    "• An\tinstance\tcan\tchange\tcluster\t(move\tto\tanother\tcluster)\twhen\tthe\tcentroids\tare\trecomputed.\t\t\n",
    "\n",
    "Disavantages\n",
    "\n",
    "• Difficult\tto\tpredict\tthe\tnumber\tof\tclusters\t(K-Value)\t\n",
    "• IniHal\tseeds\thave\ta\tstrong\timpact\ton\tthe\tfinal\tresults\t\n",
    "• The\torder\tof\tthe\tdata\thas\tan\timpact\ton\tthe\tfinal\tresults\t\n",
    "• SensiHve\tto\tscale:\trescaling\tyour\tdatasets\t(normalizaHon\tor\tstandardizaHon)\twill\t\n",
    "completely\tchange\tresults.\tWhile\tthis\titself\tis\tnot\tbad,\tnot\trealizing\tthat\tyou\thave\tto\t\n",
    "spend\textra\ta4en(on\tto\tscaling\tyour\tdata\tmight\tbe\tbad.\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8be5cd-c4fc-4cf1-85b4-e0596f37285f",
   "metadata": {},
   "source": [
    "#### 8. Draw a diagram to demonstrate the principle of clustering.\n",
    "<IMG SRC='https://miro.medium.com/max/1276/1*h1Y5k0ZSXZQmks-WjKubtQ.jpeg'></IMG>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24182654-acd5-4ce2-8850-db0e84a912c2",
   "metadata": {},
   "source": [
    "### 9. During your study, you discovered seven findings, which are listed in the data points below. Using\n",
    "the K-means algorithm, you want to build three clusters from these observations. The clusters C1,\n",
    "C2, and C3 have the following findings after the first iteration:\n",
    "\n",
    "\n",
    "C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
    "\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
    "\n",
    "C3: (5,5) and (9,9)\n",
    "\n",
    "What would the cluster centroids be if you were to run a second iteration? What would this\n",
    "clustering&#39;s SSE be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
