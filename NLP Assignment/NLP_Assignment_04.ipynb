{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc44854",
   "metadata": {},
   "source": [
    "# Assignment 04 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35eaef1",
   "metadata": {},
   "source": [
    "#### 1.\tCan you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN? And a vector-to-sequence RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79783c2d",
   "metadata": {},
   "source": [
    "In Sequence to Sequence Learning, RNN is trained to map an input sequence to an output sequence which is not necessarily of the same length. Applications are \n",
    "* speech recognition, \n",
    "* machine translation, \n",
    "* image captioning and\n",
    "* questiMon answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c478edb2",
   "metadata": {},
   "source": [
    "#### 2.\tWhy do people use encoder–decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d250f70",
   "metadata": {},
   "source": [
    "The encoder-decoder architecture for recurrent neural networks is the standard neural machine translation method that rivals and in some cases outperforms classical statistical machine translation methods.\n",
    "\n",
    "This architecture is very new, having only been pioneered in 2014, although, has been adopted as the core technology inside Google’s translate service.\n",
    "\n",
    "The two seminal examples of the encoder-decoder model for neural machine translation.\n",
    "\n",
    "\n",
    "\n",
    "* The encoder-decoder recurrent neural network architecture is the core technology inside Google’s translate service.\n",
    "* The so-called “Sutskever model” for direct end-to-end machine translation.\n",
    "* The so-called “Cho model” that extends the architecture with GRU units and an attention mechanism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb759d62",
   "metadata": {},
   "source": [
    "#### 3.\tHow could you combine a convolutional neural network with an RNN to classify videos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d535e",
   "metadata": {},
   "source": [
    "A video consists of an ordered sequence of frames. Each frame contains spatial information, and the sequence of those frames contains temporal information. To model both of these aspects, we use a hybrid architecture that consists of convolutions (for spatial processing) as well as recurrent layers (for temporal processing). Specifically, we'll use a Convolutional Neural Network (CNN) and a Recurrent Neural Network (RNN) consisting of GRU layers. This kind of hybrid architecture is popularly known as a CNN-RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639b7b3",
   "metadata": {},
   "source": [
    "#### 4.\tWhat are the advantages of building an RNN using dynamic_rnn() rather than static_rnn()?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d0300",
   "metadata": {},
   "source": [
    "**Static**\n",
    "\n",
    "Internally, tf.nn.rnn creates an unrolled graph for a fixed RNN length. That means, if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 RNN steps. First, graph creation is slow. Second, you’re unable to pass in longer sequences (> 200) than you’ve originally specified.\n",
    "\n",
    "**Dynamic**\n",
    "\n",
    "tf.nn.dynamic_rnn solves this. It uses a tf.While loop to dynamically construct the graph when it is executed. That means graph creation is faster and you can feed batches of variable size.\n",
    "\n",
    "In general he concludes that there is no real benefit in using tf.nn.static_rnn and that for most cases you'll want to resort to tf.nn.dynamic_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d0589",
   "metadata": {},
   "source": [
    "#### 5.\tHow can you deal with variable-length input sequences? What about variable-length output sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32071284",
   "metadata": {},
   "source": [
    "Deep learning libraries assume a vectorized representation of your data.\n",
    "\n",
    "In the case of variable length sequence prediction problems, this requires that your data be transformed such that each sequence has the same length.\n",
    "\n",
    "This vectorization allows code to efficiently perform the matrix operations in batch for your chosen deep learning algorithms.\n",
    "\n",
    "We can use to prepare your variable length sequence data for sequence prediction problems in Python with Keras.\n",
    "\n",
    "\n",
    "\n",
    "* How to pad variable length sequences with dummy values.\n",
    "* How to pad variable length sequences to a new longer desired length.\n",
    "* How to truncate variable length sequences to a shorter desired length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa3100",
   "metadata": {},
   "source": [
    "#### 6.\tWhat is a common way to distribute training and execution of a deep RNN across multiple GPUs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c40dd12",
   "metadata": {},
   "source": [
    "It is quite common to stack multiple layers of cells, as shown in Figure 4-12. This gives you a deep RNN.\n",
    "\n",
    "To implement a deep RNN in TensorFlow, you can create several cells and stack them into a MultiRNNCell. In the following code we stack three identical cells (but you could very well use various kinds of cells with a different number of neurons):\n",
    "\n",
    "    n_neurons = 100\n",
    "    n_layers = 3\n",
    "\n",
    "    layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons,\n",
    "                                        activation=tf.nn.relu)\n",
    "            for layer in range(n_layers)]\n",
    "    multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "    outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
    "    mlst 1412\n",
    "Figure \n",
    "<img src=\"https://www.oreilly.com/library/view/neural-networks-and/9781492037354/assets/mlst_1412.png\"></img>\n",
    " Deep RNN (left), unrolled through time (right)\n",
    "That’s all there is to it! The states variable is a tuple containing one tensor per layer, each representing the final state of that layer’s cell (with shape [batch_size, n_neurons]). If you set state_is_tuple=False when creating the MultiRNNCell, then states becomes a single tensor containing the states from every layer, concatenated along the column axis (i.e., its shape is [batch_size, n_layers * n_neurons]). Note that before TensorFlow 0.11.0, this behavior was the default."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc07d24e2f18896857f0b2a651fe84ba40ce7b297e58d8804a308c8039f752a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
